{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pa\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style \n",
    "import csv\n",
    "import glob, os\n",
    "import re\n",
    "from scipy.stats import uniform\n",
    "import math\n",
    "plt.style.use('seaborn') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66000.0\n",
      "70000\n",
      "70000\n",
      "35000\n",
      "35000\n"
     ]
    }
   ],
   "source": [
    "no_of_measurements = 6000000\n",
    "no_of_partitions = 30\n",
    "\n",
    "temp = no_of_measurements//no_of_partitions*0.33\n",
    "#print(no_of_servers_test)\n",
    "#print (int(math.ceil(no_of_servers_test/5000)*5000))\n",
    "\n",
    "no_of_servers = int(math.ceil(temp/5000)*5000)\n",
    "no_of_racks = no_of_servers\n",
    "no_of_centers = no_of_racks//2\n",
    "no_of_city = no_of_centers\n",
    "no_of_components = 4\n",
    "\n",
    "multi = 1\n",
    "\n",
    "print(temp)\n",
    "print (no_of_servers)\n",
    "print (no_of_racks)\n",
    "print(no_of_centers)\n",
    "print(no_of_city)\n",
    "\n",
    "\n",
    "#Prelim dataset\n",
    "#no_of_measurements = 200000\n",
    "#no_of_partitions = 20\n",
    "#no_of_racks = 10000\n",
    "#no_of_racks = no_of_servers\n",
    "#no_of_centers = no_of_racks//2\n",
    "#no_of_city = no_of_centers\n",
    "#no_of_components = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61.75320236, 62.69654537, 64.17672478, ..., 64.69350739,\n",
       "       55.54390296, 57.98531322])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing some new stuff here\n",
    "def move_distribution(total,no_of_partitions):\n",
    "    i=0\n",
    "    strides_up = np.geomspace(60,105,no_of_partitions/2)\n",
    "    strides_down = np.flip(strides_up-1)\n",
    "    strides= np.append(strides_up,strides_down)\n",
    "    partitions = np.ones((no_of_partitions,total//no_of_partitions))\n",
    "    for partition in partitions:\n",
    "        partition*=np.random.normal(strides[i],3,len(partition))\n",
    "        i +=1\n",
    "    return partitions.flatten()#Output 1d array. \n",
    "\n",
    "move_distribution(100000,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to create the datasets\n",
    "float_formatter = lambda x: \"%.4f\" % x\n",
    "\n",
    "def move_distribution(total,no_of_partitions):\n",
    "    i=0\n",
    "    strides_up = np.geomspace(60,105,no_of_partitions/2)\n",
    "    strides_down = np.flip(strides_up-1)\n",
    "    strides= np.append(strides_up,strides_down)\n",
    "    partitions = np.ones((no_of_partitions,total//no_of_partitions))\n",
    "    for partition in partitions:\n",
    "        partition*=np.random.normal(strides[i],3,len(partition))\n",
    "        i +=1\n",
    "    return partitions.flatten()#Output 1d array. \n",
    "\n",
    "def create_measurement (multiplier):\n",
    "    msm_id = np.arange(no_of_measurements*multiplier)+1 #Shifting to first id beeing 1\n",
    "    msm_datetime = np.round(np.linspace(start=1546300800,stop=1546300800+(no_of_measurements*multiplier/100),num=no_of_measurements*multiplier),1) \n",
    "    #tran_from_acc_id = np.floor((np.random.beta(2,8,size=no_of_transactions*multiplier)*no_of_accounts*multiplier)+1) #Shifting to match the account id's\n",
    "    #tran_from_acc_id_2 = np.floor((np.random.beta(6,8,size=no_of_transactions*multiplier//2)*no_of_accounts*multiplier)+1) #Shifting to match the account id's\n",
    "    #tran_from_acc = np.append(tran_from_acc_id_1,tran_from_acc_id_2)\n",
    "    #tran_to_acc_id = np.floor((np.random.beta(8,2,size=no_of_transactions*multiplier)*no_of_accounts*multiplier)+1) #Shifting to match the account id's\n",
    "    msm_value = np.round(move_distribution(no_of_measurements,no_of_partitions),4)#(np.random.normal(30,5,no_of_measurements*multiplier),4)\n",
    "    #msm_value = np.round(np.random.beta(40,1,size=no_of_measurements*multiplier)*100,2)\n",
    "    #tran_amount = np.floor((np.random.beta(2,8,size=no_of_transactions*multiplier)*4)+1) #Shifting to avoid transactions with 0 amount\n",
    "    msm_component_id = np.random.randint(1,no_of_components+1,no_of_measurements)\n",
    "    msm_serv_id = np.random.randint(1,no_of_servers+1,no_of_measurements)\n",
    "    #tran_prod_id = np.floor((np.random.beta(4,6,size=no_of_transactions*multiplier)*no_of_products*multiplier)+1) #Shifting to match the department id's\n",
    "    \n",
    "    df = pa.DataFrame({\n",
    "   'msm_id':msm_id, \n",
    "    'msm_datetime':msm_datetime,\n",
    "    'msm_value':msm_value,\n",
    "    'msm_component_id':msm_component_id,\n",
    "    'msm_serv_id':msm_serv_id\n",
    "    })  \n",
    "    df = df.astype({'msm_id':int,'msm_datetime':float,'msm_value':float,'msm_component_id':int,'msm_serv_id':int})\n",
    "    return df\n",
    "    \n",
    "\n",
    "def create_server(multiplier):\n",
    "    serv_id = np.arange(no_of_servers*multiplier)+1\n",
    "    serv_rack_id = np.random.randint(1,no_of_racks+1,no_of_servers)\n",
    "    df = pa.DataFrame({\n",
    "        'serv_id':serv_id,\n",
    "        'serv_rack_id':serv_rack_id\n",
    "    })\n",
    "    df = df.astype({'serv_id':int,'serv_rack_id':int})\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_rack(multiplier):\n",
    "    rack_id = np.arange(no_of_racks*multiplier)+1\n",
    "    rack_cent_id = np.random.randint(1,no_of_centers+1,no_of_racks)\n",
    "    df = pa.DataFrame({\n",
    "        'rack_id':rack_id,\n",
    "        'rack_cent_id':rack_cent_id\n",
    "    })\n",
    "    df = df.astype({'rack_id':int,'rack_cent_id':int})\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_center(multiplier):\n",
    "    cent_id = np.arange(no_of_centers*multiplier)+1\n",
    "    cent_city_id = np.random.randint(1,no_of_city+1,no_of_centers)\n",
    "    df = pa.DataFrame({\n",
    "        'cent_id':cent_id,\n",
    "        'cent_city_id':cent_city_id\n",
    "    })\n",
    "    df = df.astype({'cent_id':int,'cent_city_id':int})\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_city(multiplier):\n",
    "    city_id = np.arange(no_of_city*multiplier)+1\n",
    "    city_name = np.random.randint(1,4,no_of_city*multiplier)\n",
    "    df = pa.DataFrame({\n",
    "        'city_id':city_id,\n",
    "        'city_name':city_name\n",
    "    })\n",
    "    df = df.astype({'city_id':int,'city_name':int})\n",
    "    return df\n",
    "\n",
    "def create_component(multiplier):\n",
    "    component_id = np.arange(no_of_components*multiplier)+1\n",
    "    component_name = np.flip(component_id)\n",
    "    df = pa.DataFrame({\n",
    "        'component_id':component_id,\n",
    "        'component_name':component_name\n",
    "    })\n",
    "    df = df.astype({'component_id':int,'component_name':int})\n",
    "    return df\n",
    "\n",
    "#Wrapping function for creation of the dataset\n",
    "def create_dataset(multi=1):\n",
    "    msm_df = create_measurement(multi)\n",
    "    server_df = create_server(multi)\n",
    "    rack_df = create_rack(multi)\n",
    "    center_df = create_center(multi)\n",
    "    city_df = create_city(multi)\n",
    "    component_df = create_component(multi)\n",
    "    return msm_df,server_df,rack_df,center_df,city_df,component_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean dataset directory first\n",
    "dataset_paths = 'datasets/*'\n",
    "files = glob.glob(dataset_paths)\n",
    "for file in files:\n",
    "    os.remove(file)\n",
    "    \n",
    "#Create dataset files    \n",
    "msm_df,server_df,rack_df,center_df,city_df,component_df = create_dataset(1)\n",
    "\n",
    "# Creating the inserting sql for measurements dataset. \n",
    "msm_df['insert_string']='insert into measurement (msm_datetime,msm_value,msm_component_id,msm_serv_id) values(from_unixtime('+msm_df['msm_datetime'].map(str)+'), '+msm_df['msm_value'].map(str)+','+msm_df['msm_component_id'].map(str)+','+msm_df['msm_serv_id'].map(str)+');'\n",
    "msm_df['insert_delete_string']= 'insert into measurement (msm_datetime,msm_value,msm_component_id,msm_serv_id) values(from_unixtime('+msm_df['msm_datetime'].map(str)+'), '+msm_df['msm_value'].map(str)+','+msm_df['msm_component_id'].map(str)+','+msm_df['msm_serv_id'].map(str)+'); DELETE FROM measurement LIMIT 1;'\n",
    "\n",
    "#Dividing the measurement datasets into several files. \n",
    "partition_length = len(msm_df.index)//no_of_partitions\n",
    "msm_df['insert_string'][:partition_length].to_csv('datasets/unformatted_measurement_0.sql',index=False,header=False)\n",
    "for i in range(1,no_of_partitions+1):\n",
    "    file_name = 'unformatted_measurement_{}.sql'.format(i)\n",
    "    msm_df['insert_delete_string'][(i)*partition_length:partition_length*(i+1)].to_csv('datasets/'+file_name,index=False,header=False)\n",
    "\n",
    "    \n",
    "#Formatting the measurement.sql files to ready them for import. \n",
    "unformatted_measurement_files = sorted(glob.glob(dataset_paths))\n",
    "for file in unformatted_measurement_files:\n",
    "    output_file_name=re.sub('^datasets/unformatted_','datasets/',file)\n",
    "    with open(file, 'r') as f, open(output_file_name, 'w+') as fo:\n",
    "#      #  fo.write(\"set autocommit = 0;\")\n",
    "        for line in f:\n",
    "            fo.write(line.replace('\"', '').replace(\"'\", \"\")) \n",
    "#       # fo.write(\"COMMIT;\")\n",
    "    os.remove(file)\n",
    "    \n",
    "#Adding in the rest of the dataset as csv files. \n",
    "server_df.to_csv('datasets/server.csv',index=False)\n",
    "rack_df.to_csv('datasets/rack.csv',index=False)\n",
    "center_df.to_csv('datasets/center.csv',index=False)\n",
    "city_df.to_csv('datasets/city.csv',index=False)\n",
    "component_df.to_csv('datasets/component.csv',index=False)\n",
    "\n",
    "#Creating a copy of the data in the required file location to load into mysql. \n",
    "#We dont do this anymore, we use a symlink to the dataset location now ! cp datasets/* /home/svestrhe/Documents/Forprosjekt/code/mysql-server/mysql-test/std_data/\n",
    "#!scp -rp datasets svestrhe@loki12:/export/home/tmp/bygging/mysql-test/var/std_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "sys_var_delete_weight = 3\n",
    "no_between_updates = 10000\n",
    "rule_3_counter = 0\n",
    "\n",
    "rule_3_counter += 1*5\n",
    "print(rule_3_counter % no_between_updates < 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ax=sns.distplot(msm_df['msm_value'])\n",
    "#ax.set_xlabel(\"Value\")\n",
    "#ax.set_ylabel(\"Precentage of occurences\")\n",
    "#plt.title('Measurements')\n",
    "#Prelim_test_ds = plt.gcf()\n",
    "#Prelim_test_ds.savefig(\"/export/home/tmp/Dropbox/Apper/ShareLaTeX/Master/plots/prelim/Prelim_test_ds.png\",dpi=360)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#sns.distplot(msm_df['msm_serv_id'])\n",
    "#plt.title('Measurements_server_id')\n",
    "#plt.show()\n",
    "\n",
    "#no_of_depts=5000\n",
    "#no_of_locations=5000\n",
    "#no_of_emps=20000\n",
    "#dept_df = create_dept(1)\n",
    "#sns.distplot(dept_df['dept_name'])\n",
    "#plt.title('Department')\n",
    "#plt.show()\n",
    "\n",
    "#daa_list = [msm_df[:95000][\"msm_value\"],msm_df[93000:97000][\"msm_value\"]]\n",
    "#sns.lineplot(data=msm_df[\"msm_value\"])\n",
    "#line=sns.lineplot(x=\"msm_datetime\", y=\"msm_value\",data=msm_df[99000:])\n",
    "#plt.show()\n",
    "\n",
    "#daa_list = [msm_df[:95000][\"msm_value\"],msm_df[93000:97000][\"msm_value\"]]\n",
    "#sns.lineplot(data=msm_df[\"msm_value\"][:50000])\n",
    "#line=sns.lineplot(x=\"msm_datetime\", y=\"msm_value\",data=msm_df[99000:])\n",
    "#plt.show()\n",
    "\n",
    "#sns.distplot(msm_df[\"msm_value\"])\n",
    "#plt.show()\n",
    "\n",
    "#sns.distplot(msm_df[\"msm_value\"][:50000])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show graphs of the distributions we want to look at. \n",
    "#plt.xlim(0,len(tran_df.index))\n",
    "#sns.distplot(msm_df['msm_value'][:5000])\n",
    "#plt.title('Measurements')\n",
    "#plt.show()\n",
    "\n",
    "#sns.distplot(msm_df['msm_value'][5000:10000])\n",
    "#plt.title('Measurements')\n",
    "#plt.show()\n",
    "\n",
    "#sns.distplot(msm_df['msm_value'][:2000])\n",
    "#plt.title('Measurements')\n",
    "#plt.show()\n",
    "\n",
    "#sns.distplot(msm_df['msm_value'][2000:4000])\n",
    "#plt.title('Measurements')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#daa_list = [msm_df[:95000][\"msm_value\"],msm_df[93000:97000][\"msm_value\"]]\n",
    "#sns.lineplot(data=msm_df[\"msm_value\"])\n",
    "#line=sns.lineplot(x=\"msm_datetime\", y=\"msm_value\",data=msm_df[99000:])\n",
    "#plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax=sns.lineplot(x=\"msm_id\", y=\"msm_value\",data=msm_df)\n",
    "xlabels = ['{:,.0f}'.format(x) + 'K' for x in ax.get_xticks()/1000]\n",
    "ax.set_xticklabels(xlabels)\n",
    "plt.title(\"\")\n",
    "ax.set_xlabel(\"Row number\")\n",
    "ax.set_ylabel(\"Measurement value\")\n",
    "#plt.axvline(x=0)\n",
    "#plt.axvline(x=-2,ymax=0.25)\n",
    "#plt.axvline(x=2,ymax=0.25)\n",
    "msm_dist_plt = plt.gcf()\n",
    "msm_dist_plt.savefig(\"/export/home/tmp/Dropbox/Apper/ShareLaTeX/Master/eval_plots/msm_distribution_plt.png\",dpi=360,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dept (multiplier):\n",
    "    dept_id = np.arange(no_of_depts*multiplier) +1 #Shifting to first id beeing 1\n",
    "    #dept_name = np.random.randint(50,size=no_of_depts)\n",
    "    #dept_location = np.random.randint(50,size=no_of_depts)\n",
    "    #dept_name = np.random.chisquare(3,size=no_of_depts*multiplier)+1\n",
    "    dept_name = np.floor(np.random.beta(1,160,size=no_of_depts*multiplier)*no_of_depts*multiplier)\n",
    "    dept_location = np.random.randint(no_of_locations*multiplier,size=no_of_depts*multiplier)+1\n",
    "    dept_manager = np.random.randint(no_of_emps*multiplier,size=no_of_depts*multiplier) +1 #Can't have 0, there is no emp id = 0. \n",
    "    df= pa.DataFrame({\n",
    "    'dept_id':dept_id, \n",
    "    'dept_name':dept_name,\n",
    "    'dept_location':dept_location,\n",
    "    'dept_manager':dept_manager\n",
    "    })\n",
    "    df = df.astype({'dept_id':int,'dept_name':int,'dept_location':int,'dept_manager':int})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_location(multiplier):\n",
    "    location_id=np.arange(no_of_locations*multiplier) +1 #shifting to first id being 1\n",
    "    location_name=dept_name = np.round(np.random.beta(2,8,size=no_of_locations*multiplier)*no_of_locations*multiplier,0)\n",
    "    location_country_id = np.floor(np.random.beta(2,8,size=no_of_locations*multiplier)*no_of_countries*multiplier)+1 #Shifting to match the department id's\n",
    "    df= pa.DataFrame({\n",
    "    'location_id':location_id, \n",
    "    'location_name':location_name,\n",
    "    'location_country_id':location_country_id\n",
    "    })\n",
    "    df = df.astype ({'location_id':int,'location_name':int,'location_country_id':int})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country(multiplier):\n",
    "    country_id=np.arange(no_of_countries*multiplier)+1\n",
    "    country_name=np.floor((np.random.beta(2,8,size=no_of_countries*multiplier)*no_of_countries*multiplier)+1) #Shifting to match the department id's\n",
    "\n",
    "    df=pa.DataFrame({\n",
    "        'country_id':country_id,\n",
    "        'country_name':country_name\n",
    "    })\n",
    "    df = df.astype({'country_id':int,'country_name':int})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapping function for creation of the dataset\n",
    "def create_dataset(multi=1):\n",
    "    emp_df = create_emp(multi)\n",
    "    dept_df = create_dept(multi)\n",
    "    loc_df = create_location(multi)\n",
    "    country_df = create_country(multi)\n",
    "    return emp_df,dept_df,loc_df,country_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a run of our dataset and show the distributions we want to look at. \n",
    "\n",
    "emp_df,dept_df,loc_df,country_df = create_dataset() \n",
    "emp_df_2,dept_df_2,loc_df_2,country_df_2 = create_dataset(2) \n",
    "emp_df_4,dept_df_4,loc_df_4,country_df_4 = create_dataset(4) \n",
    "emp_df_6,dept_df_6,loc_df_6,country_df_6 = create_dataset(6) \n",
    "emp_df_8,dept_df_8,loc_df_8,country_df_8 = create_dataset(8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the datasets to the csv files. \n",
    "emp_df.to_csv('../code/mysql-server/mysql-test/std_data/employee.csv',index=False)\n",
    "dept_df.to_csv('../code/mysql-server/mysql-test/std_data/department.csv',index=False)\n",
    "loc_df.to_csv('../code/mysql-server/mysql-test/std_data/location.csv',index=False)\n",
    "country_df.to_csv('../code/mysql-server/mysql-test/std_data/country.csv',index=False)\n",
    "\n",
    "emp_df.to_csv('employee.csv',index=False)\n",
    "dept_df.to_csv('department.csv',index=False)\n",
    "loc_df.to_csv('location.csv',index=False)\n",
    "country_df.to_csv('country.csv',index=False)\n",
    "\n",
    "emp_df_2.to_csv('../code/mysql-server/mysql-test/std_data/employee_2.csv',index=False)\n",
    "dept_df_2.to_csv('../code/mysql-server/mysql-test/std_data/department_2.csv',index=False)\n",
    "loc_df_2.to_csv('../code/mysql-server/mysql-test/std_data/location_2.csv',index=False)\n",
    "country_df_2.to_csv('../code/mysql-server/mysql-test/std_data/country_2.csv',index=False)\n",
    "\n",
    "emp_df_2.to_csv('employee_2.csv',index=False)\n",
    "dept_df_2.to_csv('department_2.csv',index=False)\n",
    "loc_df_2.to_csv('location_2.csv',index=False)\n",
    "country_df_2.to_csv('country_2.csv',index=False)\n",
    "\n",
    "emp_df_4.to_csv('../code/mysql-server/mysql-test/std_data/employee_4.csv',index=False)\n",
    "dept_df_4.to_csv('../code/mysql-server/mysql-test/std_data/department_4.csv',index=False)\n",
    "loc_df_4.to_csv('../code/mysql-server/mysql-test/std_data/location_4.csv',index=False)\n",
    "country_df_4.to_csv('../code/mysql-server/mysql-test/std_data/country_4.csv',index=False)\n",
    "\n",
    "emp_df_4.to_csv('employee_4.csv',index=False)\n",
    "dept_df_4.to_csv('department_4.csv',index=False)\n",
    "loc_df_4.to_csv('location_4.csv',index=False)\n",
    "country_df_4.to_csv('country_4.csv',index=False)\n",
    "\n",
    "emp_df_6.to_csv('../code/mysql-server/mysql-test/std_data/employee_6.csv',index=False)\n",
    "dept_df_6.to_csv('../code/mysql-server/mysql-test/std_data/department_6.csv',index=False)\n",
    "loc_df_6.to_csv('../code/mysql-server/mysql-test/std_data/location_6.csv',index=False)\n",
    "country_df_6.to_csv('../code/mysql-server/mysql-test/std_data/country_6.csv',index=False)\n",
    "\n",
    "emp_df_6.to_csv('employee_6.csv',index=False)\n",
    "dept_df_6.to_csv('department_6.csv',index=False)\n",
    "loc_df_6.to_csv('location_6.csv',index=False)\n",
    "country_df_6.to_csv('country_6.csv',index=False)\n",
    "\n",
    "emp_df_8.to_csv('../code/mysql-server/mysql-test/std_data/employee_8.csv',index=False)\n",
    "dept_df_8.to_csv('../code/mysql-server/mysql-test/std_data/department_8.csv',index=False)\n",
    "loc_df_8.to_csv('../code/mysql-server/mysql-test/std_data/location_8.csv',index=False)\n",
    "country_df_8.to_csv('../code/mysql-server/mysql-test/std_data/country_8.csv',index=False)\n",
    "\n",
    "emp_df_8.to_csv('employee_8.csv',index=False)\n",
    "dept_df_8.to_csv('department_8.csv',index=False)\n",
    "loc_df_8.to_csv('location_8.csv',index=False)\n",
    "country_df_8.to_csv('country_8.csv',index=False)\n",
    "\n",
    "!sh run_tests.sh\n",
    "\n",
    "\n",
    "#emp_dfx2.to_csv('../code/mysql-server/mysql-test/std_data/same_dist_employee.csv',index=False)\n",
    "#dept_dfx2.to_csv('../code/mysql-server/mysql-test/std_data/same_dist_dept.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show graphs of the distributions we want to look at. \n",
    "plt.xlim(0,len(dept_df.index))\n",
    "sns.distplot(dept_df['dept_name'])\n",
    "plt.title('Department')\n",
    "plt.show()\n",
    "\n",
    "plt.xlim(0, len(dept_df.index))\n",
    "sns.distplot(emp_df['department_id'])\n",
    "plt.title('Employee')\n",
    "plt.show()\n",
    "\n",
    "plt.xlim(0,len(country_df.index))\n",
    "sns.distplot(country_df['country_name'])\n",
    "plt.title('Country')\n",
    "plt.show()\n",
    "\n",
    "plt.xlim(0,len(loc_df.index))\n",
    "sns.distplot(loc_df['location_name'])\n",
    "plt.title('Location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show graphs of the distributions we want to look at. \n",
    "plt.xlim(0,len(dept_df_4.index))\n",
    "sns.distplot(dept_df_4['dept_name'])\n",
    "plt.title('Department')\n",
    "plt.show()\n",
    "\n",
    "plt.xlim(0, len(dept_df_4.index))\n",
    "sns.distplot(emp_df_4['department_id'])\n",
    "plt.title('Employee')\n",
    "plt.show()\n",
    "\n",
    "plt.xlim(0,len(country_df_4.index))\n",
    "sns.distplot(country_df_4['country_name'])\n",
    "plt.title('Country')\n",
    "plt.show()\n",
    "\n",
    "plt.xlim(0,len(loc_df_4.index))\n",
    "sns.distplot(loc_df_4['location_name'])\n",
    "plt.title('Location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
